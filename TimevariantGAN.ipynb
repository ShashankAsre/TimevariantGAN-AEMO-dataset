{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TimevariantGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cciyvmjsc_jJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Code Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \n",
        "\"Time-series Generative Adversarial Networks,\" \n",
        "Neural Information Processing Systems (NeurIPS), 2019.\n",
        "Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
        "\n",
        "\n",
        "We would like to thank the authors, 'Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar'; for making 'TimeGAN' code publicly available. \n",
        "TimeGAN framework is baseline for our research on Timeseries Generation using AEMO dataset.\n",
        "\n",
        "We have perform below modification to execute the implementation of timeseries generation using AEMO-energy consumption dataset. \n",
        "New Dataset: Energy Consumption data obtained from AEMO website\n",
        "Modified Optimizer: RMSPropOptimizer\n",
        "Modified Activation function\n",
        "Added new metrics-\n",
        "  Accuracy\n",
        "  precision\n",
        "  recall\n",
        "  f1-score\n",
        "  cohens kappa\n",
        "  ROC AUC\n",
        "\n",
        "Author: Shashank Asre, Dr. Adnan Anwar\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install  tensorflow==1.15"
      ],
      "metadata": {
        "id": "8hpwxtmLdNh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.Import Necessary Classes"
      ],
      "metadata": {
        "id": "Pj9-FO2V00cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import tensorflow.compat.v1 as tf\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "from tabulate import tabulate\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Nx750iNwdY53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pre-process energy data"
      ],
      "metadata": {
        "id": "QGo7VcwJ3lzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MinMaxScaler(data):\n",
        "  norm_data = (data - np.min(data, 0)) / (np.max(data, 0) - np.min(data, 0))\n",
        "  return norm_data"
      ],
      "metadata": {
        "id": "Q7mRhQ1L57Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def real_data_loading (data, seq_len):\n",
        "  ori_data= data\n",
        "  # Flip the data to make chronological data\n",
        "  ori_data = ori_data[::-1]\n",
        "  # Normalize the data\n",
        "  ori_data = MinMaxScaler(ori_data)\n",
        "    \n",
        "  # Preprocess the dataset\n",
        "  temp_data = []    \n",
        "  # Cut data by sequence length\n",
        "  for i in range(0, len(ori_data) - seq_len):\n",
        "    _x = ori_data[i:i + seq_len]\n",
        "    temp_data.append(_x)\n",
        "        \n",
        "  # Mix the datasets (to make it similar to i.i.d)\n",
        "  idx = np.random.permutation(len(temp_data))    \n",
        "  data = []\n",
        "  for i in range(len(temp_data)):\n",
        "    data.append(temp_data[idx[i]])\n",
        "    \n",
        "  return data"
      ],
      "metadata": {
        "id": "0_a-xnXd3qhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "\n",
        "energy_data = np.loadtxt('energy_data.csv', delimiter = \",\",skiprows = 1)"
      ],
      "metadata": {
        "id": "ZmRMA2HRdbJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 24\n",
        "ori_data = real_data_loading(energy_data, seq_len)"
      ],
      "metadata": {
        "id": "CsUF4s915qxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define Utilities class"
      ],
      "metadata": {
        "id": "iznflrZw6PHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_divide (data_x, data_x_hat, data_t, data_t_hat, train_rate = 0.8):\n",
        "\n",
        "  # Divide train/test index (original data)\n",
        "  no = len(data_x)\n",
        "  idx = np.random.permutation(no)\n",
        "  train_idx = idx[:int(no*train_rate)]\n",
        "  test_idx = idx[int(no*train_rate):]\n",
        "    \n",
        "  train_x = [data_x[i] for i in train_idx]\n",
        "  test_x = [data_x[i] for i in test_idx]\n",
        "  train_t = [data_t[i] for i in train_idx]\n",
        "  test_t = [data_t[i] for i in test_idx]      \n",
        "    \n",
        "  # Divide train/test index (synthetic data)\n",
        "  no = len(data_x_hat)\n",
        "  idx = np.random.permutation(no)\n",
        "  train_idx = idx[:int(no*train_rate)]\n",
        "  test_idx = idx[int(no*train_rate):]\n",
        "  \n",
        "  train_x_hat = [data_x_hat[i] for i in train_idx]\n",
        "  test_x_hat = [data_x_hat[i] for i in test_idx]\n",
        "  train_t_hat = [data_t_hat[i] for i in train_idx]\n",
        "  test_t_hat = [data_t_hat[i] for i in test_idx]\n",
        "  \n",
        "  return train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat\n",
        "\n",
        "\n",
        "def extract_time (data):\n",
        " \n",
        "  time = list()\n",
        "  max_seq_len = 0\n",
        "  for i in range(len(data)):\n",
        "    max_seq_len = max(max_seq_len, len(data[i][:,0]))\n",
        "    time.append(len(data[i][:,0]))\n",
        "    \n",
        "  return time, max_seq_len\n",
        "\n",
        "\n",
        "def rnn_cell(module_name, hidden_dim):\n",
        "\n",
        "  assert module_name in ['gru','lstm','lstmLN']\n",
        "  \n",
        "  # GRU\n",
        "  if (module_name == 'gru'):\n",
        "    rnn_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
        "  # LSTM\n",
        "  elif (module_name == 'lstm'):\n",
        "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
        "  # LSTM Layer Normalization\n",
        "  elif (module_name == 'lstmLN'):\n",
        "    rnn_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
        "  return rnn_cell\n",
        "\n",
        "\n",
        "def random_generator (batch_size, z_dim, T_mb, max_seq_len):\n",
        " \n",
        "  Z_mb = list()\n",
        "  for i in range(batch_size):\n",
        "    temp = np.zeros([max_seq_len, z_dim])\n",
        "    temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n",
        "    temp[:T_mb[i],:] = temp_Z\n",
        "    Z_mb.append(temp_Z)\n",
        "  return Z_mb\n",
        "\n",
        "\n",
        "def batch_generator(data, time, batch_size):\n",
        "\n",
        "  no = len(data)\n",
        "  idx = np.random.permutation(no)\n",
        "  train_idx = idx[:batch_size]     \n",
        "            \n",
        "  X_mb = list(data[i] for i in train_idx)\n",
        "  T_mb = list(time[i] for i in train_idx)\n",
        "  \n",
        "  return X_mb, T_mb"
      ],
      "metadata": {
        "id": "joPZZpJT6gXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define TimevariantGAN"
      ],
      "metadata": {
        "id": "3VmvU4k_7Jmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def timegan (ori_data, parameters):\n",
        "  # Initialization on the Graph\n",
        "  ops.reset_default_graph()\n",
        "\n",
        "  # Basic Parameters\n",
        "  no, seq_len, dim = np.asarray(ori_data).shape\n",
        "    \n",
        "  # Maximum sequence length and each sequence length\n",
        "  ori_time, max_seq_len = extract_time(ori_data)\n",
        "  \n",
        "  def MinMaxScaler1(data):\n",
        "   \n",
        "    min_val = np.min(np.min(data, axis = 0), axis = 0)\n",
        "    data = data - min_val\n",
        "      \n",
        "    max_val = np.max(np.max(data, axis = 0), axis = 0)\n",
        "    norm_data = data / (max_val + 1e-7)\n",
        "      \n",
        "    return norm_data, min_val, max_val\n",
        "  \n",
        "  # Normalization\n",
        "  ori_data, min_val, max_val = MinMaxScaler1(ori_data)\n",
        "              \n",
        "  ## Build a RNN networks          \n",
        "  \n",
        "  # Network Parameters\n",
        "  hidden_dim   = parameters['hidden_dim'] \n",
        "  num_layers   = parameters['num_layer']\n",
        "  iterations   = parameters['iterations']\n",
        "  batch_size   = parameters['batch_size']\n",
        "  module_name  = parameters['module'] \n",
        "  z_dim        = dim\n",
        "  gamma        = 1\n",
        "    \n",
        "  # Input place holders\n",
        "  #tf.disable_v2_behavior()\n",
        "  X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x\")\n",
        "  Z = tf.placeholder(tf.float32, [None, max_seq_len, z_dim], name = \"myinput_z\")\n",
        "  T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
        "  \n",
        "  def embedder (X, T):\n",
        "\n",
        "    with tf.variable_scope(\"embedder\", reuse = tf.AUTO_REUSE):\n",
        "      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
        "      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, X, dtype=tf.float32, sequence_length = T)\n",
        "      tf.disable_v2_behavior()\n",
        "      H = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
        "    return H\n",
        "      \n",
        "  def recovery (H, T):   \n",
        "    \n",
        "    with tf.variable_scope(\"recovery\", reuse = tf.AUTO_REUSE):       \n",
        "      r_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
        "      r_outputs, r_last_states = tf.nn.dynamic_rnn(r_cell, H, dtype=tf.float32, sequence_length = T)\n",
        "      X_tilde = tf.contrib.layers.fully_connected(r_outputs, dim, activation_fn=tf.nn.sigmoid) \n",
        "    return X_tilde\n",
        "    \n",
        "  def generator (Z, T):  \n",
        "       \n",
        "    with tf.variable_scope(\"generator\", reuse = tf.AUTO_REUSE):\n",
        "      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
        "      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, Z, dtype=tf.float32, sequence_length = T)\n",
        "      E = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
        "    return E\n",
        "      \n",
        "  def supervisor (H, T): \n",
        "            \n",
        "    with tf.variable_scope(\"supervisor\", reuse = tf.AUTO_REUSE):\n",
        "      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers-1)])\n",
        "      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, H, dtype=tf.float32, sequence_length = T)\n",
        "      S = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)     \n",
        "    return S\n",
        "          \n",
        "  def discriminator (H, T):\n",
        "            \n",
        "    with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE):\n",
        "      d_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
        "      d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, H, dtype=tf.float32, sequence_length = T)\n",
        "      Y_hat = tf.contrib.layers.fully_connected(d_outputs, 1, activation_fn=None) \n",
        "    return Y_hat   \n",
        "    \n",
        "  # Embedder & Recovery\n",
        "  H = embedder(X, T)\n",
        "  X_tilde = recovery(H, T)\n",
        "    \n",
        "  # Generator\n",
        "  E_hat = generator(Z, T)\n",
        "  H_hat = supervisor(E_hat, T)\n",
        "  H_hat_supervise = supervisor(H, T)\n",
        "    \n",
        "  # Synthetic data\n",
        "  X_hat = recovery(H_hat, T)\n",
        "    \n",
        "  # Discriminator\n",
        "  Y_fake = discriminator(H_hat, T)\n",
        "  Y_real = discriminator(H, T)     \n",
        "  Y_fake_e = discriminator(E_hat, T)\n",
        "    \n",
        "  # Variables        \n",
        "  e_vars = [v for v in tf.trainable_variables() if v.name.startswith('embedder')]\n",
        "  r_vars = [v for v in tf.trainable_variables() if v.name.startswith('recovery')]\n",
        "  g_vars = [v for v in tf.trainable_variables() if v.name.startswith('generator')]\n",
        "  s_vars = [v for v in tf.trainable_variables() if v.name.startswith('supervisor')]\n",
        "  d_vars = [v for v in tf.trainable_variables() if v.name.startswith('discriminator')]\n",
        "    \n",
        "  # Discriminator loss\n",
        "  D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)\n",
        "  D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)\n",
        "  D_loss_fake_e = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)\n",
        "  D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
        "            \n",
        "  # Generator loss\n",
        "  # 1. Adversarial loss\n",
        "  G_loss_U = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake), Y_fake)\n",
        "  G_loss_U_e = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e), Y_fake_e)\n",
        "    \n",
        "  # 2. Supervised loss\n",
        "  G_loss_S = tf.losses.mean_squared_error(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
        "    \n",
        "  # 3. Two Momments\n",
        "  G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(X_hat,[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X,[0])[1] + 1e-6)))\n",
        "  G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(X_hat,[0])[0]) - (tf.nn.moments(X,[0])[0])))\n",
        "    \n",
        "  G_loss_V = G_loss_V1 + G_loss_V2\n",
        "    \n",
        "  # 4. Summation\n",
        "  G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V \n",
        "            \n",
        "  # Embedder network loss\n",
        "  E_loss_T0 = tf.losses.mean_squared_error(X, X_tilde)\n",
        "  E_loss0 = 10*tf.sqrt(E_loss_T0)\n",
        "  E_loss = E_loss0  + 0.1*G_loss_S\n",
        "    \n",
        "  # optimizer\n",
        "  E0_solver = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.9, momentum=0.01).minimize(E_loss0, var_list = e_vars + r_vars)\n",
        "  E_solver = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.9, momentum=0.01).minimize(E_loss, var_list = e_vars + r_vars)\n",
        "  D_solver = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.9, momentum=0.01).minimize(D_loss, var_list = d_vars)\n",
        "  G_solver = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.9, momentum=0.01).minimize(G_loss, var_list = g_vars + s_vars)      \n",
        "  GS_solver = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.9, momentum=0.01).minimize(G_loss_S, var_list = g_vars + s_vars)   \n",
        "        \n",
        "  ## TimeGAN training   \n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  # 1. Embedding network training\n",
        "  print('Start Embedding Network Training')\n",
        "    \n",
        "  for itt in range(iterations):\n",
        "    # Set mini-batch\n",
        "    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           \n",
        "    # Train embedder        \n",
        "    _, step_e_loss = sess.run([E0_solver, E_loss_T0], feed_dict={X: X_mb, T: T_mb})        \n",
        "    # Checkpoint\n",
        "    if itt % 1000 == 0:\n",
        "      print('step: '+ str(itt) + '/' + str(iterations) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss),4)) ) \n",
        "      \n",
        "  print('Finish Embedding Network Training')\n",
        "    \n",
        "  # 2. Training only with supervised loss\n",
        "  print('Start Training with Supervised Loss Only')\n",
        "    \n",
        "  for itt in range(iterations):\n",
        "    # Set mini-batch\n",
        "    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)    \n",
        "    # Random vector generation   \n",
        "    Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
        "    # Train generator       \n",
        "    _, step_g_loss_s = sess.run([GS_solver, G_loss_S], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})       \n",
        "    # Checkpoint\n",
        "    if itt % 1000 == 0:\n",
        "      print('step: '+ str(itt)  + '/' + str(iterations) +', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s),4)) )\n",
        "      \n",
        "  print('Finish Training with Supervised Loss Only')\n",
        "    \n",
        "  # 3. Joint Training\n",
        "  print('Start Joint Training')\n",
        "  \n",
        "  for itt in range(iterations):\n",
        "    # Generator training (twice more than discriminator training)\n",
        "    for kk in range(2):\n",
        "      # Set mini-batch\n",
        "      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)               \n",
        "      # Random vector generation\n",
        "      Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
        "      # Train generator\n",
        "      _, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run([G_solver, G_loss_U, G_loss_S, G_loss_V], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})\n",
        "       # Train embedder        \n",
        "      _, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})   \n",
        "           \n",
        "    # Discriminator training        \n",
        "    # Set mini-batch\n",
        "    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           \n",
        "    # Random vector generation\n",
        "    Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
        "    # Check discriminator loss before updating\n",
        "    check_d_loss = sess.run(D_loss, feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
        "    # Train discriminator (only when the discriminator does not work well)\n",
        "    if (check_d_loss > 0.15):        \n",
        "      _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
        "        \n",
        "    # Print multiple checkpoints\n",
        "    if itt % 1000 == 0:\n",
        "      print('step: '+ str(itt) + '/' + str(iterations) + \n",
        "            ', d_loss: ' + str(np.round(step_d_loss,4)) + \n",
        "            ', g_loss_u: ' + str(np.round(step_g_loss_u,4)) + \n",
        "            ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s),4)) + \n",
        "            ', g_loss_v: ' + str(np.round(step_g_loss_v,4)) + \n",
        "            ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0),4))  )\n",
        "  print('Finish Joint Training')\n",
        "    \n",
        "  ## Synthetic data generation\n",
        "  Z_mb = random_generator(no, z_dim, ori_time, max_seq_len)\n",
        "  generated_data_curr = sess.run(X_hat, feed_dict={Z: Z_mb, X: ori_data, T: ori_time})    \n",
        "    \n",
        "  generated_data = list()\n",
        "    \n",
        "  for i in range(no):\n",
        "    temp = generated_data_curr[i,:ori_time[i],:]\n",
        "    generated_data.append(temp)\n",
        "        \n",
        "  # Renormalization\n",
        "  generated_data = generated_data * max_val\n",
        "  generated_data = generated_data + min_val\n",
        "    \n",
        "  return generated_data"
      ],
      "metadata": {
        "id": "G7ZjlGts7NR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Initialize parameters and Execute TimevariantGAN"
      ],
      "metadata": {
        "id": "v0KOinWz8XWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Newtork parameters\n",
        "parameters = dict()\n",
        "\n",
        "parameters['module'] = 'gru' \n",
        "parameters['hidden_dim'] = 24\n",
        "parameters['num_layer'] = 3\n",
        "parameters['iterations'] = 10000\n",
        "parameters['batch_size'] = 64"
      ],
      "metadata": {
        "id": "KzgsDjYQ8UZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run TimeGAN\n",
        "generated_data = timegan(ori_data, parameters)   \n",
        "print('Finish Synthetic Data Generation')"
      ],
      "metadata": {
        "id": "DYeYMxyO8jn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Define Metrices"
      ],
      "metadata": {
        "id": "5A7qS0Q69b05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitative metrics"
      ],
      "metadata": {
        "id": "JpV9DkVa9gt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminative_score_metrics (ori_data, generated_data):\n",
        "\n",
        "  # Initialization on the Graph\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # Basic Parameters\n",
        "  no, seq_len, dim = np.asarray(ori_data).shape    \n",
        "    \n",
        "  # Set maximum sequence length and each sequence length\n",
        "  ori_time, ori_max_seq_len = extract_time(ori_data)\n",
        "  generated_time, generated_max_seq_len = extract_time(ori_data)\n",
        "  max_seq_len = max([ori_max_seq_len, generated_max_seq_len])  \n",
        "     \n",
        "  ## Builde a post-hoc RNN discriminator network \n",
        "  # Network parameters\n",
        "  hidden_dim = int(dim/2)\n",
        "  iterations = 2000\n",
        "  batch_size = 128\n",
        "    \n",
        "  # Input place holders\n",
        "  # Feature\n",
        "  X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x\")\n",
        "  X_hat = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x_hat\")\n",
        "    \n",
        "  T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
        "  T_hat = tf.placeholder(tf.int32, [None], name = \"myinput_t_hat\")\n",
        "    \n",
        "  # discriminator function\n",
        "  def discriminator (x, t):\n",
        "   \n",
        "    with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE) as vs:\n",
        "      d_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'd_cell')\n",
        "      d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, x, dtype=tf.float32, sequence_length = t)\n",
        "      y_hat_logit = tf.contrib.layers.fully_connected(d_last_states, 1, activation_fn=None) \n",
        "      y_hat = tf.nn.sigmoid(y_hat_logit)\n",
        "      d_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
        "    \n",
        "    return y_hat_logit, y_hat, d_vars\n",
        "    \n",
        "  y_logit_real, y_pred_real, d_vars = discriminator(X, T)\n",
        "  y_logit_fake, y_pred_fake, _ = discriminator(X_hat, T_hat)\n",
        "        \n",
        "  # Loss for the discriminator\n",
        "  d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = y_logit_real, \n",
        "                                                                       labels = tf.ones_like(y_logit_real)))\n",
        "  d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = y_logit_fake, \n",
        "                                                                       labels = tf.zeros_like(y_logit_fake)))\n",
        "  d_loss = d_loss_real + d_loss_fake\n",
        "    \n",
        "  # optimizer\n",
        "  d_solver = tf.train.RMSPropOptimizer(learning_rate =0.001, decay=0.9, momentum=0.1).minimize(d_loss, var_list = d_vars)\n",
        "        \n",
        "  ## Train the discriminator   \n",
        "  # Start session and initialize\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  # Train/test division for both original and generated data\n",
        "  train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat = \\\n",
        "  train_test_divide(ori_data, generated_data, ori_time, generated_time)\n",
        "    \n",
        "  # Training step\n",
        "  for itt in range(iterations):\n",
        "          \n",
        "    # Batch setting\n",
        "    X_mb, T_mb = batch_generator(train_x, train_t, batch_size)\n",
        "    X_hat_mb, T_hat_mb = batch_generator(train_x_hat, train_t_hat, batch_size)\n",
        "          \n",
        "    # Train discriminator\n",
        "    _, step_d_loss = sess.run([d_solver, d_loss], \n",
        "                              feed_dict={X: X_mb, T: T_mb, X_hat: X_hat_mb, T_hat: T_hat_mb})            \n",
        "    \n",
        "  ## Test the performance on the testing set    \n",
        "  y_pred_real_curr, y_pred_fake_curr = sess.run([y_pred_real, y_pred_fake], \n",
        "                                                feed_dict={X: test_x, T: test_t, X_hat: test_x_hat, T_hat: test_t_hat})\n",
        "    \n",
        "  y_pred_final = np.squeeze(np.concatenate((y_pred_real_curr, y_pred_fake_curr), axis = 0))\n",
        "  y_label_final = np.concatenate((np.ones([len(y_pred_real_curr),]), np.zeros([len(y_pred_fake_curr),])), axis = 0)\n",
        "    \n",
        "  # Compute the accuracy\n",
        "  acc = accuracy_score(y_label_final, (y_pred_final>0.5))\n",
        "  pre = precision_score(y_label_final, (y_pred_final>0.5))\n",
        "  re = recall_score(y_label_final, (y_pred_final>0.5))\n",
        "  f1 = f1_score(y_label_final, (y_pred_final>0.5))\n",
        "  kappa = cohen_kappa_score(y_label_final, (y_pred_final>0.5))\n",
        "  auc = roc_auc_score(y_label_final, (y_pred_final>0.5))\n",
        "  discriminative_score = np.abs(0.5-acc)\n",
        "    \n",
        "  return discriminative_score, acc, pre, re, f1, kappa, auc"
      ],
      "metadata": {
        "id": "GGp_jSHR8kmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictive_score_metrics (ori_data, generated_data):\n",
        "\n",
        "  # Initialization on the Graph\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # Basic Parameters\n",
        "  no, seq_len, dim = np.asarray(ori_data).shape\n",
        "    \n",
        "  # Set maximum sequence length and each sequence length\n",
        "  ori_time, ori_max_seq_len = extract_time(ori_data)\n",
        "  generated_time, generated_max_seq_len = extract_time(ori_data)\n",
        "  max_seq_len = max([ori_max_seq_len, generated_max_seq_len])  \n",
        "     \n",
        "  ## Builde a post-hoc RNN predictive network \n",
        "  # Network parameters\n",
        "  hidden_dim = int(dim/2)\n",
        "  iterations = 5000\n",
        "  batch_size = 128\n",
        "    \n",
        "  # Input place holders\n",
        "  X = tf.placeholder(tf.float32, [None, max_seq_len-1, dim-1], name = \"myinput_x\")\n",
        "  T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")    \n",
        "  Y = tf.placeholder(tf.float32, [None, max_seq_len-1, 1], name = \"myinput_y\")\n",
        "    \n",
        "  # Predictor function\n",
        "  def predictor (x, t):\n",
        "\n",
        "    with tf.variable_scope(\"predictor\", reuse = tf.AUTO_REUSE) as vs:\n",
        "      p_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'p_cell')\n",
        "      p_outputs, p_last_states = tf.nn.dynamic_rnn(p_cell, x, dtype=tf.float32, sequence_length = t)\n",
        "      y_hat_logit = tf.contrib.layers.fully_connected(p_outputs, 1, activation_fn=None) \n",
        "      y_hat = tf.nn.sigmoid(y_hat_logit)\n",
        "      p_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
        "    \n",
        "    return y_hat, p_vars\n",
        "    \n",
        "  y_pred, p_vars = predictor(X, T)\n",
        "  # Loss for the predictor\n",
        "  p_loss = tf.losses.absolute_difference(Y, y_pred)\n",
        "  # optimizer\n",
        "  p_solver = tf.train.RMSPropOptimizer(learning_rate =0.001, decay=0.9, momentum=0.01).minimize(p_loss, var_list = p_vars)\n",
        "        \n",
        "  ## Training    \n",
        "  # Session start\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "  # Training using Synthetic dataset\n",
        "  for itt in range(iterations):\n",
        "          \n",
        "    # Set mini-batch\n",
        "    idx = np.random.permutation(len(generated_data))\n",
        "    train_idx = idx[:batch_size]     \n",
        "            \n",
        "    X_mb = list(generated_data[i][:-1,:(dim-1)] for i in train_idx)\n",
        "    T_mb = list(generated_time[i]-1 for i in train_idx)\n",
        "    Y_mb = list(np.reshape(generated_data[i][1:,(dim-1)],[len(generated_data[i][1:,(dim-1)]),1]) for i in train_idx)        \n",
        "          \n",
        "    # Train predictor\n",
        "    _, step_p_loss = sess.run([p_solver, p_loss], feed_dict={X: X_mb, T: T_mb, Y: Y_mb})        \n",
        "    \n",
        "  ## Test the trained model on the original data\n",
        "  idx = np.random.permutation(len(ori_data))\n",
        "  train_idx = idx[:no]\n",
        "    \n",
        "  X_mb = list(ori_data[i][:-1,:(dim-1)] for i in train_idx)\n",
        "  T_mb = list(ori_time[i]-1 for i in train_idx)\n",
        "  Y_mb = list(np.reshape(ori_data[i][1:,(dim-1)], [len(ori_data[i][1:,(dim-1)]),1]) for i in train_idx)\n",
        "    \n",
        "  # Prediction\n",
        "  pred_Y_curr = sess.run(y_pred, feed_dict={X: X_mb, T: T_mb})\n",
        "    \n",
        "  # Compute the performance in terms of MAE\n",
        "  MAE_temp = 0\n",
        "  for i in range(no):\n",
        "    MAE_temp = MAE_temp + mean_absolute_error(Y_mb[i], pred_Y_curr[i,:,:])\n",
        "    \n",
        "  predictive_score = MAE_temp / no\n",
        "    \n",
        "  return predictive_score"
      ],
      "metadata": {
        "id": "FJZeLbZ_99fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization Metrics"
      ],
      "metadata": {
        "id": "l8pFht6z_5Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualization (ori_data, generated_data, analysis):\n",
        "   \n",
        "  # Analysis sample size (for faster computation)\n",
        "  anal_sample_no = min([1000, len(ori_data)])\n",
        "  idx = np.random.permutation(len(ori_data))[:anal_sample_no]\n",
        "    \n",
        "  # Data preprocessing\n",
        "  ori_data = np.asarray(ori_data)\n",
        "  generated_data = np.asarray(generated_data)  \n",
        "  \n",
        "  ori_data = ori_data[idx]\n",
        "  generated_data = generated_data[idx]\n",
        "  \n",
        "  no, seq_len, dim = ori_data.shape  \n",
        "  \n",
        "  for i in range(anal_sample_no):\n",
        "    if (i == 0):\n",
        "      prep_data = np.reshape(np.mean(ori_data[0,:,:], 1), [1,seq_len])\n",
        "      prep_data_hat = np.reshape(np.mean(generated_data[0,:,:],1), [1,seq_len])\n",
        "    else:\n",
        "      prep_data = np.concatenate((prep_data, \n",
        "                                  np.reshape(np.mean(ori_data[i,:,:],1), [1,seq_len])))\n",
        "      prep_data_hat = np.concatenate((prep_data_hat, \n",
        "                                      np.reshape(np.mean(generated_data[i,:,:],1), [1,seq_len])))\n",
        "    \n",
        "  # Visualization parameter        \n",
        "  colors = [\"green\" for i in range(anal_sample_no)] + [\"red\" for i in range(anal_sample_no)]    \n",
        "    \n",
        "  if analysis == 'pca':\n",
        "    # PCA Analysis\n",
        "    pca = PCA(n_components = 2)\n",
        "    pca.fit(prep_data)\n",
        "    pca_results = pca.transform(prep_data)\n",
        "    pca_hat_results = pca.transform(prep_data_hat)\n",
        "    \n",
        "    # Plotting\n",
        "    f, ax = plt.subplots(1)    \n",
        "    plt.scatter(pca_results[:,0], pca_results[:,1],\n",
        "                c = colors[:anal_sample_no], alpha = 0.2, label = \"Original\")\n",
        "    plt.scatter(pca_hat_results[:,0], pca_hat_results[:,1], \n",
        "                c = colors[anal_sample_no:], alpha = 0.2, label = \"Synthetic\")\n",
        "  \n",
        "    ax.legend()  \n",
        "    plt.title('PCA plot')\n",
        "    plt.xlabel('x-pca')\n",
        "    plt.ylabel('y_pca')\n",
        "    plt.show()\n",
        "    \n",
        "  elif analysis == 'tsne':\n",
        "    \n",
        "    # Do t-SNE Analysis together       \n",
        "    prep_data_final = np.concatenate((prep_data, prep_data_hat), axis = 0)\n",
        "    \n",
        "    # TSNE anlaysis\n",
        "    tsne = TSNE(n_components = 2, verbose = 1, perplexity = 40, n_iter = 300)\n",
        "    tsne_results = tsne.fit_transform(prep_data_final)\n",
        "      \n",
        "    # Plotting\n",
        "    f, ax = plt.subplots(1)\n",
        "      \n",
        "    plt.scatter(tsne_results[:anal_sample_no,0], tsne_results[:anal_sample_no,1], \n",
        "                c = colors[:anal_sample_no], alpha = 0.2, label = \"Original\")\n",
        "    plt.scatter(tsne_results[anal_sample_no:,0], tsne_results[anal_sample_no:,1], \n",
        "                c = colors[anal_sample_no:], alpha = 0.2, label = \"Synthetic\")\n",
        "  \n",
        "    ax.legend()\n",
        "      \n",
        "    plt.title('t-SNE plot')\n",
        "    plt.xlabel('x-tsne')\n",
        "    plt.ylabel('y_tsne')\n",
        "    plt.show()  "
      ],
      "metadata": {
        "id": "i8nr6u-eATXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Calculate Metrics"
      ],
      "metadata": {
        "id": "ovveiHxoArYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantitative Evaluation"
      ],
      "metadata": {
        "id": "ApMqJnUUDzNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_iteration = 5\n",
        "\n",
        "discriminative_score = list()\n",
        "pre_score = list()\n",
        "rec_score = list()\n",
        "f_score = list()\n",
        "kappa_score = list()\n",
        "auc_score = list()\n",
        "acc_score = list()\n",
        "\n",
        "for _ in range(metric_iteration):\n",
        "  temp_disc, temp_acc, temp_precision, temp_recall, temp_f1, temp_kappa, temp_auc = discriminative_score_metrics(ori_data, generated_data)\n",
        "  discriminative_score.append(temp_disc)\n",
        "  acc_score.append(temp_acc)\n",
        "  pre_score.append(temp_precision)\n",
        "  rec_score.append(temp_recall)\n",
        "  f_score.append(temp_f1)\n",
        "  kappa_score.append(temp_kappa)\n",
        "  auc_score.append(temp_auc)"
      ],
      "metadata": {
        "id": "7c5_cFRAApn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictive_score = list()\n",
        "for tt in range(metric_iteration):\n",
        "  temp_pred = predictive_score_metrics(ori_data, generated_data)\n",
        "  predictive_score.append(temp_pred)   \n"
      ],
      "metadata": {
        "id": "tw-Aw5bMA7gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data = [[1, 'Discriminative score', np.round(np.mean(discriminative_score), 4)],\n",
        "[2, 'Accuracy', np.round(np.mean(acc_score), 4)],\n",
        "[3, 'Precision', np.round(np.mean(pre_score), 4)],\n",
        "[4,'Recall', np.round(np.mean(rec_score), 4)],\n",
        "[5,'F1-Score', np.round(np.mean(f_score), 4)],\n",
        "[6,'Cohens kappa', np.round(np.mean(kappa_score), 4)],\n",
        "[7,'ROC AUC', np.round(np.mean(auc_score), 4)],\n",
        "[8,'Predictive score', np.round(np.mean(predictive_score), 4)]] \n",
        "print(tabulate(eval_data, headers=[\"Sr No.\", \"Metric\", \"Score\"]))"
      ],
      "metadata": {
        "id": "ZpHVoujuCgpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics on Original data and generated data"
      ],
      "metadata": {
        "id": "-JQjL6wAC9IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_sample = min([1000, len(ori_data)])\n",
        "idx_1 = np.random.permutation(len(ori_data))[:x_sample]\n",
        "    \n",
        "  # Data preprocessing\n",
        "ori_data1 = np.asarray(ori_data)\n",
        "generated_data1 = np.asarray(generated_data)  \n",
        "  \n",
        "ori_data2 = ori_data1[idx_1]\n",
        "generated_data2 = generated_data1[idx_1]\n",
        "  \n",
        "no, seq_len, dim = ori_data2.shape  \n",
        "  \n",
        "for i in range(x_sample):\n",
        "  if (i == 0):\n",
        "    prep_data1 = np.reshape(np.mean(ori_data2[0,:,:], 1), [1,seq_len])\n",
        "    prep_data_hat1 = np.reshape(np.mean(generated_data2[0,:,:],1), [1,seq_len])\n",
        "  else:\n",
        "    prep_data1 = np.concatenate((prep_data1,np.reshape(np.mean(ori_data2[i,:,:],1), [1,seq_len])))\n",
        "    prep_data_hat1 = np.concatenate((prep_data_hat1,np.reshape(np.mean(generated_data2[i,:,:],1), [1,seq_len])))"
      ],
      "metadata": {
        "id": "HFg_1c1MDB-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Statistic on subset of Original Data\n",
        "df_describe = pd.DataFrame(np.asarray(prep_data1))\n",
        "stat= df_describe[0].describe()\n",
        "stat = stat.iloc[1:]\n",
        "stat"
      ],
      "metadata": {
        "id": "iBbBWjnfDHHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Statistics on subset of Synthetic data\n",
        "df_describe1 = pd.DataFrame(np.asarray(prep_data_hat1))\n",
        "stat1= df_describe1[0].describe()\n",
        "stat1 = stat1.iloc[1:]\n",
        "stat1"
      ],
      "metadata": {
        "id": "5LaLq1AlDkbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "_DCJLHxID3Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Visualize original and synthetic data using PCA and TSNE\n",
        "visualization(ori_data, generated_data, 'pca')\n",
        "visualization(ori_data, generated_data, 'tsne')"
      ],
      "metadata": {
        "id": "usgVQ-CaD48w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}